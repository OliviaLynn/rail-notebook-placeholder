Goldenspike: an example of an end-to-end analysis using RAIL
============================================================

**Authors:** Sam Schmidt, Eric Charles, Alex Malz, John Franklin
Crenshaw, others…

**Last run successfully:** Oct 23, 2023

This notebook demonstrates how to use a the various RAIL Modules to draw
synthetic samples of fluxes by color, apply physical effects to them,
train photo-Z estimators on the samples, test and validate the
preformance of those estimators, and to use the RAIL summarization
modules to obtain n(z) estimates based on the p(z) estimates.

**Creation**

Note that in the parlance of the Creation Module, “degradation” is any
post-processing that occurs to the “true” sample generated by the create
Engine. This can include adding photometric errors, applying quality
cuts, introducing systematic biases, etc.

In this notebook, we will draw both test and training samples from a
RAIL Engine object. Then we will demonstrate how to use RAIL degraders
to apply effects to those samples.

**Training and Estimation**

The RAIL Informer modules “train” or “inform” models used to estimate
p(z) given band fluxes (and potentially other information).

The RAIL Estimation modules then use those same models to actually apply
the model and extract the p(z) estimates.

**p(z) Validation**

The RAIL Validator module applies various metrics.

**p(z) to n(z) Summarization**

The RAIL Summarization modules convert per-galaxy p(z) posteriors to
ensemble n(z) estimates.

Imports
-------

.. code:: ipython3

    # Prerquisites: os, numpy, pathlib, pzflow, tables_io
    import os
    import numpy as np
    from pathlib import Path
    from collections import OrderedDict
    from pzflow.examples import get_galaxy_data
    import tables_io


.. code:: ipython3

    # Various rail modules
    import rail
    from rail.creation.degraders.photometric_errors import LSSTErrorModel
    from rail.creation.degraders.spectroscopic_degraders import (
        InvRedshiftIncompleteness,
        LineConfusion,
    )
    from rail.creation.degraders.quantityCut import QuantityCut
    from rail.creation.engines.flowEngine import FlowModeler, FlowCreator, FlowPosterior
    from rail.core.data import TableHandle
    from rail.core.stage import RailStage
    from rail.tools.table_tools import ColumnMapper, TableConverter
    
    from rail.estimation.algos.bpz_lite import BPZliteInformer, BPZliteEstimator
    from rail.estimation.algos.k_nearneigh import KNearNeighInformer, KNearNeighEstimator
    from rail.estimation.algos.flexzboost import FlexZBoostInformer, FlexZBoostEstimator
    
    from rail.estimation.algos.naive_stack import NaiveStackSummarizer
    from rail.estimation.algos.point_est_hist import PointEstHistSummarizer
    
    from rail.evaluation.dist_to_point_evaluator import DistToPointEvaluator
    


RAIL now uses ceci as a back-end, which takes care of a lot of file I/O
decisions to be consistent with other choices in DESC.

The data_store commands in the cell below effectively override a ceci
default to prevent overwriting previous results, generally good but not
necessary for this demo.

The ``DataStore`` uses ``DataHandle`` objects to keep track of the
connections between the various stages. When one stage returns a
``DataHandle`` and then you pass that ``DataHandle`` to another stage,
the underlying code can establish the connections needed to build a
reproducilble pipeline.

.. code:: ipython3

    DS = RailStage.data_store
    DS.__class__.allow_overwrite = True


Here we need a few configuration parameters to deal with differences in
data schema between existing PZ codes.

.. code:: ipython3

    bands = ["u", "g", "r", "i", "z", "y"]
    band_dict = {band: f"mag_{band}_lsst" for band in bands}
    rename_dict = {f"mag_{band}_lsst_err": f"mag_err_{band}_lsst" for band in bands}


Train the Flow Engine
---------------------

First we need to train the normalizing flow that will serve as the
engine for the notebook.

In the cell below, we load the example galaxy catalog from PZFlow and
save it so that it can be used to train the flow. We also set the path
where we will save the flow.

.. code:: ipython3

    DATA_DIR = Path().resolve() / "data"
    DATA_DIR.mkdir(exist_ok=True)
    
    catalog_file = DATA_DIR / "base_catalog.pq"
    catalog = get_galaxy_data().rename(band_dict, axis=1)
    tables_io.write(catalog, str(catalog_file.with_suffix("")), catalog_file.suffix[1:])
    
    catalog_file = str(catalog_file)
    flow_file = str(DATA_DIR / "trained_flow.pkl")


Now we set the parameters for the FlowModeler, i.e. the pipeline stage
that trains the flow:

.. code:: ipython3

    flow_modeler_params = {
        "name": "flow_modeler",
        "input": catalog_file,
        "model": flow_file,
        "seed": 0,
        "phys_cols": {"redshift": [0, 3]},
        "phot_cols": {
            "mag_u_lsst": [17, 35],
            "mag_g_lsst": [16, 32],
            "mag_r_lsst": [15, 30],
            "mag_i_lsst": [15, 30],
            "mag_z_lsst": [14, 29],
            "mag_y_lsst": [14, 28],
        },
        "calc_colors": {"ref_column_name": "mag_i_lsst"},
    }


Now we will create the flow and train it

.. code:: ipython3

    flow_modeler = FlowModeler.make_stage(**flow_modeler_params)


.. code:: ipython3

    flow_modeler.fit_model()



.. parsed-literal::

    Inserting handle into data store.  input: /home/runner/work/rail_notebooks/rail_notebooks/rail/examples/goldenspike_examples/data/base_catalog.pq, flow_modeler
    column_list None
    Training 30 epochs 
    Loss:


.. parsed-literal::

    (0) 21.3266


.. parsed-literal::

    (1) 5.2449


.. parsed-literal::

    (2) 3.3295


.. parsed-literal::

    (3) 2.0386


.. parsed-literal::

    (4) -0.5743


.. parsed-literal::

    (5) -0.0965


.. parsed-literal::

    (6) 0.2187


.. parsed-literal::

    (7) -0.8406


.. parsed-literal::

    (8) -1.2236


.. parsed-literal::

    (9) -1.2393


.. parsed-literal::

    (10) -1.7110


.. parsed-literal::

    (11) -1.7557


.. parsed-literal::

    (12) -1.2808


.. parsed-literal::

    (13) -0.6698


.. parsed-literal::

    (14) -2.7405


.. parsed-literal::

    (15) -2.6874


.. parsed-literal::

    (16) -1.4010


.. parsed-literal::

    (17) -2.7131


.. parsed-literal::

    (18) -2.2422


.. parsed-literal::

    (19) -1.9770


.. parsed-literal::

    (20) -1.3173


.. parsed-literal::

    (21) -0.4368


.. parsed-literal::

    (22) -2.1433


.. parsed-literal::

    (23) -2.6517


.. parsed-literal::

    (24) -3.3043


.. parsed-literal::

    (25) -1.6357


.. parsed-literal::

    (26) -2.9331


.. parsed-literal::

    (27) -1.8108


.. parsed-literal::

    (28) -3.1526


.. parsed-literal::

    (29) -3.6577


.. parsed-literal::

    (30) -4.2410
    Inserting handle into data store.  model_flow_modeler: /home/runner/work/rail_notebooks/rail_notebooks/rail/examples/goldenspike_examples/data/inprogress_trained_flow.pkl, flow_modeler




.. parsed-literal::

    <rail.tools.flow_handle.FlowHandle at 0x7fd363563400>



Make mock data
--------------

Now we will use the trained flow to create training and test data for
the photo-z estimators.

For both the training and test data we will:

1. Use the Flow to produce some synthetic data
2. Use the LSSTErrorModel to add photometric errors
3. Use the FlowPosterior to estimate the redshift posteriors for the
   degraded sample
4. Use the ColumnMapper to rename the error columns so that they match
   the names in DC2.
5. Use the TableConverter to convert the data to a numpy dictionary,
   which will be stored in a hdf5 file with the same schema as the DC2
   data

Training sample
~~~~~~~~~~~~~~~

For the training data we are going to apply a couple of extra
degradation effects to the data beyond what we do to create test data,
as the training data will have some spectroscopic incompleteness. This
will allow us to see how the trained models perform with imperfect
training data.

More details about the degraders are available in the
``rail/examples/creation_examples/degradation_demo.ipynb`` notebook.

.. code:: ipython3

    flow_creator_train = FlowCreator.make_stage(
        name="flow_creator_train",
        model=flow_modeler.get_handle("model"),
        n_samples=50,
        seed=1235,
    )
    
    lsst_error_model_train = LSSTErrorModel.make_stage(
        name="lsst_error_model_train",
        renameDict=band_dict,
        ndFlag=np.nan,
        seed=29,
    )
    
    inv_redshift = InvRedshiftIncompleteness.make_stage(
        name="inv_redshift",
        pivot_redshift=1.0,
    )
    
    line_confusion = LineConfusion.make_stage(
        name="line_confusion",
        true_wavelen=5007.0,
        wrong_wavelen=3727.0,
        frac_wrong=0.05,
        seed = 1337
    )
    
    quantity_cut = QuantityCut.make_stage(
        name="quantity_cut",
        cuts={"mag_i_lsst": 25.0},
    )
    
    col_remapper_train = ColumnMapper.make_stage(
        name="col_remapper_train",
        columns=rename_dict,
    )
    
    table_conv_train = TableConverter.make_stage(
        name="table_conv_train",
        output_format="numpyDict",
    )


.. code:: ipython3

    train_data_orig = flow_creator_train.sample(150, 1235)
    train_data_errs = lsst_error_model_train(train_data_orig, seed=66)
    train_data_inc = inv_redshift(train_data_errs)
    train_data_conf = line_confusion(train_data_inc)
    train_data_cut = quantity_cut(train_data_conf)
    train_data_pq = col_remapper_train(train_data_cut)
    train_data = table_conv_train(train_data_pq)



.. parsed-literal::

    Inserting handle into data store.  output_flow_creator_train: inprogress_output_flow_creator_train.pq, flow_creator_train
    Inserting handle into data store.  output_lsst_error_model_train: inprogress_output_lsst_error_model_train.pq, lsst_error_model_train
    Inserting handle into data store.  output_inv_redshift: inprogress_output_inv_redshift.pq, inv_redshift
    Inserting handle into data store.  output_line_confusion: inprogress_output_line_confusion.pq, line_confusion
    Inserting handle into data store.  output_quantity_cut: inprogress_output_quantity_cut.pq, quantity_cut
    Inserting handle into data store.  output_col_remapper_train: inprogress_output_col_remapper_train.pq, col_remapper_train
    Inserting handle into data store.  output_table_conv_train: inprogress_output_table_conv_train.hdf5, table_conv_train


Let’s examine the quantities that we’ve generated, we’ll use the handy
``tables_io`` package to temporarily write to a pandas dataframe for
quick writeout of the columns:

.. code:: ipython3

    train_table = tables_io.convertObj(train_data.data, tables_io.types.PD_DATAFRAME)
    train_table.head()





.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>redshift</th>
          <th>mag_u_lsst</th>
          <th>mag_err_u_lsst</th>
          <th>mag_g_lsst</th>
          <th>mag_err_g_lsst</th>
          <th>mag_r_lsst</th>
          <th>mag_err_r_lsst</th>
          <th>mag_i_lsst</th>
          <th>mag_err_i_lsst</th>
          <th>mag_z_lsst</th>
          <th>mag_err_z_lsst</th>
          <th>mag_y_lsst</th>
          <th>mag_err_y_lsst</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.831240</td>
          <td>26.585936</td>
          <td>0.402909</td>
          <td>26.035440</td>
          <td>0.092434</td>
          <td>24.827467</td>
          <td>0.027940</td>
          <td>23.712691</td>
          <td>0.017289</td>
          <td>23.108116</td>
          <td>0.019257</td>
          <td>22.904551</td>
          <td>0.035842</td>
        </tr>
        <tr>
          <th>1</th>
          <td>1.130516</td>
          <td>25.684467</td>
          <td>0.194579</td>
          <td>24.929354</td>
          <td>0.034793</td>
          <td>23.935784</td>
          <td>0.013262</td>
          <td>23.112760</td>
          <td>0.010854</td>
          <td>22.080541</td>
          <td>0.008912</td>
          <td>21.596383</td>
          <td>0.011992</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.725424</td>
          <td>25.900969</td>
          <td>0.233052</td>
          <td>25.897163</td>
          <td>0.081854</td>
          <td>25.567525</td>
          <td>0.053750</td>
          <td>24.932885</td>
          <td>0.050015</td>
          <td>24.926190</td>
          <td>0.095089</td>
          <td>24.601710</td>
          <td>0.159289</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.773942</td>
          <td>25.228076</td>
          <td>0.131913</td>
          <td>24.223101</td>
          <td>0.018924</td>
          <td>23.327332</td>
          <td>0.008694</td>
          <td>22.443670</td>
          <td>0.007285</td>
          <td>22.021649</td>
          <td>0.008602</td>
          <td>21.870298</td>
          <td>0.014819</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.601639</td>
          <td>25.175620</td>
          <td>0.126076</td>
          <td>24.228938</td>
          <td>0.019016</td>
          <td>23.063888</td>
          <td>0.007526</td>
          <td>22.398394</td>
          <td>0.007135</td>
          <td>22.233053</td>
          <td>0.009823</td>
          <td>22.040701</td>
          <td>0.017020</td>
        </tr>
      </tbody>
    </table>
    </div>



You see that we’ve generated redshifts, ugrizy magnitudes, and magnitude
errors with names that match those in the cosmoDC2_v1.1.4_image data.

Testing sample
~~~~~~~~~~~~~~

For the test sample we will:

1. Use the Flow to produce some synthetic data
2. Use the LSSTErrorModel to smear the data
3. Use the FlowPosterior to estimate the redshift posteriors for the
   degraded sample
4. Use ColumnMapper to rename some of the columns to match DC2
5. Use the TableConverter to convert the data to a numpy dictionary,
   which will be stored in a hdf5 file with the same schema as the DC2
   data

.. code:: ipython3

    flow_creator_test = FlowCreator.make_stage(
        name="flow_creator_test",
        model=flow_modeler.get_handle("model"),
        n_samples=50,
    )
    
    lsst_error_model_test = LSSTErrorModel.make_stage(
        name="lsst_error_model_test",
        renameDict=band_dict,
        ndFlag=np.nan,
    )
    
    flow_post_test = FlowPosterior.make_stage(
        name="flow_post_test",
        model=flow_modeler.get_handle("model"),
        column="redshift",
        grid=np.linspace(0.0, 5.0, 21),
    )
    
    col_remapper_test = ColumnMapper.make_stage(
        name="col_remapper_test",
        columns=rename_dict,
        hdf5_groupname="",
    )
    
    table_conv_test = TableConverter.make_stage(
        name="table_conv_test",
        output_format="numpyDict",
    )


.. code:: ipython3

    test_data_orig = flow_creator_test.sample(150, 1234)
    test_data_errs = lsst_error_model_test(test_data_orig, seed=58)
    test_data_post = flow_post_test.get_posterior(test_data_errs, err_samples=None)
    test_data_pq = col_remapper_test(test_data_errs)
    test_data = table_conv_test(test_data_pq)



.. parsed-literal::

    Inserting handle into data store.  output_flow_creator_test: inprogress_output_flow_creator_test.pq, flow_creator_test
    Inserting handle into data store.  output_lsst_error_model_test: inprogress_output_lsst_error_model_test.pq, lsst_error_model_test


.. parsed-literal::

    Inserting handle into data store.  output_flow_post_test: inprogress_output_flow_post_test.hdf5, flow_post_test
    Inserting handle into data store.  output_col_remapper_test: inprogress_output_col_remapper_test.pq, col_remapper_test
    Inserting handle into data store.  output_table_conv_test: inprogress_output_table_conv_test.hdf5, table_conv_test


.. parsed-literal::

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/qp/interp_pdf.py:89: RuntimeWarning: invalid value encountered in divide
      self._ycumul = (self._ycumul.T / self._ycumul[:, -1]).T


.. code:: ipython3

    test_data_handle = TableHandle('input')
    od = OrderedDict()
    od['photometry'] = test_data_orig.data.to_records()
    test_data_handle.set_data(od)
    test_data_handle.path  = './test_data_orig.hdf5'
    test_data_handle.write()




.. parsed-literal::

    './test_data_orig.hdf5'




.. code:: ipython3

    test_table = tables_io.convertObj(test_data.data, tables_io.types.PD_DATAFRAME)
    test_table.head()





.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>redshift</th>
          <th>mag_u_lsst</th>
          <th>mag_err_u_lsst</th>
          <th>mag_g_lsst</th>
          <th>mag_err_g_lsst</th>
          <th>mag_r_lsst</th>
          <th>mag_err_r_lsst</th>
          <th>mag_i_lsst</th>
          <th>mag_err_i_lsst</th>
          <th>mag_z_lsst</th>
          <th>mag_err_z_lsst</th>
          <th>mag_y_lsst</th>
          <th>mag_err_y_lsst</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.081767</td>
          <td>25.167951</td>
          <td>0.125243</td>
          <td>24.159809</td>
          <td>0.017954</td>
          <td>23.873610</td>
          <td>0.012644</td>
          <td>23.644324</td>
          <td>0.016342</td>
          <td>23.653787</td>
          <td>0.030853</td>
          <td>23.619611</td>
          <td>0.067569</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.203397</td>
          <td>23.868760</td>
          <td>0.040133</td>
          <td>23.138891</td>
          <td>0.008567</td>
          <td>22.666548</td>
          <td>0.006380</td>
          <td>22.493484</td>
          <td>0.007461</td>
          <td>22.460162</td>
          <td>0.011510</td>
          <td>22.447283</td>
          <td>0.024016</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.495480</td>
          <td>26.521034</td>
          <td>0.383230</td>
          <td>25.428170</td>
          <td>0.054075</td>
          <td>24.248755</td>
          <td>0.017043</td>
          <td>23.803431</td>
          <td>0.018647</td>
          <td>23.580394</td>
          <td>0.028929</td>
          <td>23.244698</td>
          <td>0.048451</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.526716</td>
          <td>27.255525</td>
          <td>0.657271</td>
          <td>26.808079</td>
          <td>0.180202</td>
          <td>25.654259</td>
          <td>0.058052</td>
          <td>25.175414</td>
          <td>0.062029</td>
          <td>24.893704</td>
          <td>0.092414</td>
          <td>25.192138</td>
          <td>0.261197</td>
        </tr>
        <tr>
          <th>4</th>
          <td>1.985718</td>
          <td>28.029497</td>
          <td>1.076912</td>
          <td>26.292044</td>
          <td>0.115669</td>
          <td>25.886484</td>
          <td>0.071319</td>
          <td>25.465211</td>
          <td>0.080163</td>
          <td>24.574358</td>
          <td>0.069725</td>
          <td>24.077895</td>
          <td>0.101193</td>
        </tr>
      </tbody>
    </table>
    </div>



“Inform” some estimators
------------------------

More details about the process of “informing” or “training” the models
used by the estimators is available in the
``rail/examples/estimation_examples/RAIL_estimation_demo.ipynb``
notebook.

We use “inform” rather than “train” to generically refer to the
preprocessing of any prior information. For a machine learning
estimator, that prior information is a training set, but it can also be
an SED template library for a template-fitting or hybrid estimator.

.. code:: ipython3

    inform_bpz = BPZliteInformer.make_stage(
        name="inform_bpz",
        nondetect_val=np.nan,
        model="bpz.pkl",
        hdf5_groupname="",
    )
    
    inform_knn = KNearNeighInformer.make_stage(
        name="inform_knn",
        nondetect_val=np.nan,
        model="knnpz.pkl",
        hdf5_groupname="",
    )
    
    inform_fzboost = FlexZBoostInformer.make_stage(
        name="inform_FZBoost",
        nondetect_val=np.nan,
        model="fzboost.pkl",
        hdf5_groupname="",
    )


.. code:: ipython3

    train_data_errs.data.keys()




.. parsed-literal::

    Index(['redshift', 'mag_u_lsst', 'mag_u_lsst_err', 'mag_g_lsst',
           'mag_g_lsst_err', 'mag_r_lsst', 'mag_r_lsst_err', 'mag_i_lsst',
           'mag_i_lsst_err', 'mag_z_lsst', 'mag_z_lsst_err', 'mag_y_lsst',
           'mag_y_lsst_err'],
          dtype='object')



.. code:: ipython3

    inform_bpz.inform(train_data)
    inform_knn.inform(train_data)
    inform_fzboost.inform(train_data)



.. parsed-literal::

    using 54 galaxies in calculation
    best values for fo and kt:
    [1.]
    [0.3]
    minimizing for type 0
    best fit z0, alpha, km for type 0: [0.49877582 2.1822972  0.05577249]
    Inserting handle into data store.  model_inform_bpz: inprogress_bpz.pkl, inform_bpz
    split into 40 training and 14 validation samples
    finding best fit sigma and NNeigh...


.. parsed-literal::

    
    
    
    best fit values are sigma=0.075 and numneigh=3
    
    
    
    Inserting handle into data store.  model_inform_knn: inprogress_knnpz.pkl, inform_knn
    stacking some data...
    read in training data
    fit the model...


.. parsed-literal::

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
      pid = os.fork()


.. parsed-literal::

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:31:59] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:31:59] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:31:59] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:31:59] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)


.. parsed-literal::

    finding best bump thresh...


.. parsed-literal::

    finding best sharpen parameter...


.. parsed-literal::

    Retraining with full training set...


.. parsed-literal::

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:32:03] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:32:03] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:32:03] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [07:32:03] WARNING: /workspace/src/learner.cc:740: 
    Parameters: { "silent" } are not used.
    
      warnings.warn(smsg, UserWarning)


.. parsed-literal::

    Inserting handle into data store.  model_inform_FZBoost: inprogress_fzboost.pkl, inform_FZBoost




.. parsed-literal::

    <rail.core.data.ModelHandle at 0x7fd324b4a3b0>



Estimate photo-z posteriors
---------------------------

More detail on the specific estimators used here is available in the
``rail/examples/estimation_examples/RAIL_estimation_demo.ipynb``
notebook, but here is a very brief summary of the three estimators used
in this notebook:

``BPZliteEstimator`` is a template-based photo-z code that outputs the
posterior estimated given likelihoods calculated using a template set
combined with a Bayesian prior. See Benitez (2000) for more details.
``KNearNeighEstimator`` is a simple photo-z code that finds the K
nearest neighbor training galaxies in color/magnitude space and creates
a weighted (by distance) mixture model PDF based on the redshifts of
those K neighbors. ``FlexZBoostEstimator`` is a mature photo-z algorithm
that estimates a PDF for each galaxy via a conditional density estimate
using the training data. See `Izbicki & Lee
(2017) <https://doi.org/10.1214/17-EJS1302>`__ for more details.

.. code:: ipython3

    estimate_bpz = BPZliteEstimator.make_stage(
        name="estimate_bpz",
        hdf5_groupname="",
        nondetect_val=np.nan,
        model=inform_bpz.get_handle("model"),
    )
    
    estimate_knn = KNearNeighEstimator.make_stage(
        name="estimate_knn",
        hdf5_groupname="",
        nondetect_val=np.nan,
        model=inform_knn.get_handle("model"),
    )
    
    estimate_fzboost = FlexZBoostEstimator.make_stage(
        name="test_FZBoost",
        nondetect_val=np.nan,
        model=inform_fzboost.get_handle("model"),
        hdf5_groupname="",
        aliases=dict(input="test_data", output="fzboost_estim"),
    )


.. code:: ipython3

    knn_estimated = estimate_knn.estimate(test_data)
    fzboost_estimated = estimate_fzboost.estimate(test_data)
    bpz_estimated = estimate_bpz.estimate(test_data)



.. parsed-literal::

    Process 0 running estimator on chunk 0 - 150
    Process 0 estimating PZ PDF for rows 0 - 150
    Inserting handle into data store.  output_estimate_knn: inprogress_output_estimate_knn.hdf5, estimate_knn


.. parsed-literal::

    Process 0 running estimator on chunk 0 - 150
    Process 0 estimating PZ PDF for rows 0 - 150


.. parsed-literal::

    Inserting handle into data store.  output_test_FZBoost: inprogress_output_test_FZBoost.hdf5, test_FZBoost


.. parsed-literal::

    Process 0 running estimator on chunk 0 - 150
    Inserting handle into data store.  output_estimate_bpz: inprogress_output_estimate_bpz.hdf5, estimate_bpz


Evaluate the estimates
----------------------

Now we evaluate metrics on the estimates, separately for each estimator.

Each call to the ``Evaluator.evaluate`` will create a table with the
various performance metrics. We will store all of these tables in a
dictionary, keyed by the name of the estimator.

.. code:: ipython3

    ztrue_file = './test_data_orig.hdf5'
    ztrue_data = DS.read_file('ztrue_data', TableHandle, ztrue_file)


.. code:: ipython3

    eval_dict = dict(bpz=bpz_estimated, fzboost=fzboost_estimated, knn=knn_estimated)
    
    evaluator_stage_dict = dict(
        metrics=['cdeloss', 'pit', 'brier'],
        _random_state=None,
        metric_config={
            'brier': {'limits':(0,3.1)},
            'pit':{'tdigest_compression': 1000},
        }
    )
    truth = ztrue_data
    
    result_dict = {}
    for key, val in eval_dict.items():
        the_eval = DistToPointEvaluator.make_stage(name=f'{key}_dist_to_point', force_exact=True, **evaluator_stage_dict)
        result_dict[key] = the_eval.evaluate(val, truth)
        
        
        



.. parsed-literal::

    WARNING:root:Input predictions do not sum to 1.


.. parsed-literal::

    WARNING:root:Input predictions do not sum to 1.


.. parsed-literal::

    WARNING:root:Input predictions do not sum to 1.


.. parsed-literal::

    Requested metrics: ['cdeloss', 'pit', 'brier']
    Inserting handle into data store.  output_bpz_dist_to_point: inprogress_output_bpz_dist_to_point.hdf5, bpz_dist_to_point
    Inserting handle into data store.  summary_bpz_dist_to_point: inprogress_summary_bpz_dist_to_point.hdf5, bpz_dist_to_point
    Inserting handle into data store.  single_distribution_summary_bpz_dist_to_point: inprogress_single_distribution_summary_bpz_dist_to_point.hdf5, bpz_dist_to_point
    Warning.  Failed to convert column No conversion path for dtype: dtype('<U16')
    Requested metrics: ['cdeloss', 'pit', 'brier']
    Inserting handle into data store.  output_fzboost_dist_to_point: inprogress_output_fzboost_dist_to_point.hdf5, fzboost_dist_to_point
    Inserting handle into data store.  summary_fzboost_dist_to_point: inprogress_summary_fzboost_dist_to_point.hdf5, fzboost_dist_to_point
    Inserting handle into data store.  single_distribution_summary_fzboost_dist_to_point: inprogress_single_distribution_summary_fzboost_dist_to_point.hdf5, fzboost_dist_to_point
    Warning.  Failed to convert column No conversion path for dtype: dtype('<U16')
    Requested metrics: ['cdeloss', 'pit', 'brier']
    Inserting handle into data store.  output_knn_dist_to_point: inprogress_output_knn_dist_to_point.hdf5, knn_dist_to_point
    Inserting handle into data store.  summary_knn_dist_to_point: inprogress_summary_knn_dist_to_point.hdf5, knn_dist_to_point
    Inserting handle into data store.  single_distribution_summary_knn_dist_to_point: inprogress_single_distribution_summary_knn_dist_to_point.hdf5, knn_dist_to_point
    Warning.  Failed to convert column No conversion path for dtype: dtype('<U16')


.. code:: ipython3

    result_dict




.. parsed-literal::

    {'bpz': {'output': <rail.core.data.Hdf5Handle at 0x7fd316fa8be0>,
      'summary': <rail.core.data.Hdf5Handle at 0x7fd316fbb190>,
      'single_distribution_summary': <rail.core.data.QPDictHandle at 0x7fd316fbb970>},
     'fzboost': {'output': <rail.core.data.Hdf5Handle at 0x7fd316fa2800>,
      'summary': <rail.core.data.Hdf5Handle at 0x7fd360fbf040>,
      'single_distribution_summary': <rail.core.data.QPDictHandle at 0x7fd316fba2f0>},
     'knn': {'output': <rail.core.data.Hdf5Handle at 0x7fd3385f71c0>,
      'summary': <rail.core.data.Hdf5Handle at 0x7fd316fb9690>,
      'single_distribution_summary': <rail.core.data.QPDictHandle at 0x7fd316fbb940>}}



| The Pandas DataFrame output format conveniently makes human-readable
  printouts of the metrics.
| This next cell will convert everything to Pandas.

.. code:: ipython3

    results_tables = {
        key: tables_io.convertObj(val['summary'].data, tables_io.types.PD_DATAFRAME)
        for key, val in result_dict.items()
    }


.. code:: ipython3

    results_tables["knn"]





.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cdeloss</th>
          <th>brier</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>-0.677523</td>
          <td>226.607031</td>
        </tr>
      </tbody>
    </table>
    </div>



.. code:: ipython3

    results_tables["fzboost"]





.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cdeloss</th>
          <th>brier</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>-0.277364</td>
          <td>389.690358</td>
        </tr>
      </tbody>
    </table>
    </div>



.. code:: ipython3

    results_tables["bpz"]





.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>cdeloss</th>
          <th>brier</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>3.23565</td>
          <td>857.312345</td>
        </tr>
      </tbody>
    </table>
    </div>



Summarize the per-galaxy redshift constraints to make population-level distributions
------------------------------------------------------------------------------------

{introduce the summarizers}

First we make the stages, then execute them, then plot the output.

.. code:: ipython3

    point_estimate_test = PointEstHistSummarizer.make_stage(name="point_estimate_test")
    naive_stack_test = NaiveStackSummarizer.make_stage(name="naive_stack_test")


.. code:: ipython3

    point_estimate_ens = point_estimate_test.summarize(eval_dict["bpz"])
    naive_stack_ens = naive_stack_test.summarize(eval_dict["bpz"])



.. parsed-literal::

    Process 0 running estimator on chunk 0 - 150
    Inserting handle into data store.  output_point_estimate_test: inprogress_output_point_estimate_test.hdf5, point_estimate_test
    Inserting handle into data store.  single_NZ_point_estimate_test: inprogress_single_NZ_point_estimate_test.hdf5, point_estimate_test
    Process 0 running estimator on chunk 0 - 150


.. parsed-literal::

    Inserting handle into data store.  output_naive_stack_test: inprogress_output_naive_stack_test.hdf5, naive_stack_test
    Inserting handle into data store.  single_NZ_naive_stack_test: inprogress_single_NZ_naive_stack_test.hdf5, naive_stack_test


.. code:: ipython3

    _ = naive_stack_ens.data.plot_native(xlim=(0, 3))




.. image:: ../../../docs/rendered/goldenspike_examples/goldenspike_files/../../../docs/rendered/goldenspike_examples/goldenspike_47_0.png


.. code:: ipython3

    _ = point_estimate_ens.data.plot_native(xlim=(0, 3))




.. image:: ../../../docs/rendered/goldenspike_examples/goldenspike_files/../../../docs/rendered/goldenspike_examples/goldenspike_48_0.png


Convert this to a ``ceci`` Pipeline
-----------------------------------

Now that we have all these stages defined and configured, and that we
have established the connections between them by passing ``DataHandle``
objects between them, we can build a ``ceci`` Pipeline.

.. code:: ipython3

    import ceci
    
    pipe = ceci.Pipeline.interactive()
    stages = [
        # train the flow
        flow_modeler,
        # create the training catalog
        flow_creator_train,
        lsst_error_model_train,
        inv_redshift,
        line_confusion,
        quantity_cut,
        col_remapper_train,
        table_conv_train,
        # create the test catalog
        flow_creator_test,
        lsst_error_model_test,
        col_remapper_test,
        table_conv_test,
        # inform the estimators
        inform_bpz,
        inform_knn,
        inform_fzboost,
        # estimate posteriors
        estimate_bpz,
        estimate_knn,
        estimate_fzboost,
        # estimate n(z), aka "summarize"
        point_estimate_test,
        naive_stack_test,
    ]
    for stage in stages:
        pipe.add_stage(stage)


.. code:: ipython3

    pipe.initialize(
        dict(input=catalog_file), dict(output_dir=".", log_dir=".", resume=False), None
    )





.. parsed-literal::

    (({'flow_modeler': <Job flow_modeler>,
       'flow_creator_train': <Job flow_creator_train>,
       'flow_creator_test': <Job flow_creator_test>,
       'lsst_error_model_train': <Job lsst_error_model_train>,
       'lsst_error_model_test': <Job lsst_error_model_test>,
       'inv_redshift': <Job inv_redshift>,
       'col_remapper_test': <Job col_remapper_test>,
       'line_confusion': <Job line_confusion>,
       'table_conv_test': <Job table_conv_test>,
       'quantity_cut': <Job quantity_cut>,
       'col_remapper_train': <Job col_remapper_train>,
       'table_conv_train': <Job table_conv_train>,
       'inform_bpz': <Job inform_bpz>,
       'inform_knn': <Job inform_knn>,
       'inform_FZBoost': <Job inform_FZBoost>,
       'estimate_bpz': <Job estimate_bpz>,
       'estimate_knn': <Job estimate_knn>,
       'test_FZBoost': <Job test_FZBoost>,
       'point_estimate_test': <Job point_estimate_test>,
       'naive_stack_test': <Job naive_stack_test>},
      [<rail.creation.engines.flowEngine.FlowModeler at 0x7fd37c384460>,
       <rail.creation.engines.flowEngine.FlowCreator at 0x7fd36307fb80>,
       <rail.creation.engines.flowEngine.FlowCreator at 0x7fd3384f5750>,
       <rail.creation.degraders.photometric_errors.LSSTErrorModel at 0x7fd36307e470>,
       <rail.creation.degraders.photometric_errors.LSSTErrorModel at 0x7fd36307ee90>,
       <rail.creation.degraders.spectroscopic_degraders.InvRedshiftIncompleteness at 0x7fd36307fbb0>,
       Stage that applies remaps the following column names in a pandas DataFrame:
       f{str(self.config.columns)},
       <rail.creation.degraders.spectroscopic_degraders.LineConfusion at 0x7fd36307f3d0>,
       <rail.tools.table_tools.TableConverter at 0x7fd3384f5de0>,
       Degrader that applies the following cuts to a pandas DataFrame:
       {column: (min, max), ...}
       {'mag_i_lsst': (-inf, 25.0)},
       Stage that applies remaps the following column names in a pandas DataFrame:
       f{str(self.config.columns)},
       <rail.tools.table_tools.TableConverter at 0x7fd360fbd9c0>,
       <rail.estimation.algos.bpz_lite.BPZliteInformer at 0x7fd3384f6770>,
       <rail.estimation.algos.k_nearneigh.KNearNeighInformer at 0x7fd369fd4fa0>,
       <rail.estimation.algos.flexzboost.FlexZBoostInformer at 0x7fd3cc373490>,
       <rail.estimation.algos.bpz_lite.BPZliteEstimator at 0x7fd3383846d0>,
       <rail.estimation.algos.k_nearneigh.KNearNeighEstimator at 0x7fd316fa30a0>,
       <rail.estimation.algos.flexzboost.FlexZBoostEstimator at 0x7fd316fa0790>,
       <rail.estimation.algos.point_est_hist.PointEstHistSummarizer at 0x7fd316fba440>,
       <rail.estimation.algos.naive_stack.NaiveStackSummarizer at 0x7fd316fb9cf0>]),
     {'output_dir': '.', 'log_dir': '.', 'resume': False})



.. code:: ipython3

    pipe.save("tmp_goldenspike.yml")


Read back the pipeline and run it
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    pr = ceci.Pipeline.read("tmp_goldenspike.yml")


.. code:: ipython3

    pr.run()



.. parsed-literal::

    
    Executing flow_modeler
    Command is:
    OMP_NUM_THREADS=1   python3 -m ceci rail.creation.engines.flowEngine.FlowModeler   --input=/home/runner/work/rail_notebooks/rail_notebooks/rail/examples/goldenspike_examples/data/base_catalog.pq   --name=flow_modeler   --config=tmp_goldenspike_config.yml   --model=.//home/runner/work/rail_notebooks/rail_notebooks/rail/examples/goldenspike_examples/data/trained_flow.pkl 
    Output writing to ./flow_modeler.out
    


.. parsed-literal::

    Job flow_modeler has completed successfully!


.. parsed-literal::

    
    Executing flow_creator_train
    Command is:
    OMP_NUM_THREADS=1   python3 -m ceci rail.creation.engines.flowEngine.FlowCreator   --model=.//home/runner/work/rail_notebooks/rail_notebooks/rail/examples/goldenspike_examples/data/trained_flow.pkl   --name=flow_creator_train   --config=tmp_goldenspike_config.yml   --output=./output_flow_creator_train.pq 
    Output writing to ./flow_creator_train.out
    


.. parsed-literal::

    Job flow_creator_train has failed with status 1


.. parsed-literal::

    
    *************************************************
    Error running pipeline stage flow_creator_train.
    
    Standard output and error streams in ./flow_creator_train.out
    *************************************************




.. parsed-literal::

    1



Clean up:
---------

Finally, you’ll notice that we’ve written a large number of temporary
files in the course of running this demo, to delete these and clean up
the directory just run the ``cleanup.sh`` script in this directory to
delete the data files.

.. code:: ipython3

    # TODO fix and add clean up scripts

